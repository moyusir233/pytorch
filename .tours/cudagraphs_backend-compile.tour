{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "cudagraphs_backend compile",
  "steps": [
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "注册cudagraphs后端的入口",
      "line": 145,
      "title": "注册cudagraphs后端的入口"
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "进行cugagraphs编译的函数",
      "line": 134
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "将graph module划分与重组后，再将它们利用cuda graph的接口来包装",
      "line": 136
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "将fx graph划分为若干可以在cuda graph下运行的子graph modules，其中关于SupportOperator与Partitioner的用法可以见:https://pytorch.org/docs/stable/torch.compiler_transformations.html",
      "line": 42,
      "title": "partition_cudagraphs"
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "利用FakeTensorProp捕获graph中的张量信息，具体来说，FakeTensorProp在FakeTensorMode下运行了整个fx graph，并且将每个Node经过run_node的输出保存在了node.meta['val']中",
      "line": 49
    },
    {
      "file": "torch/fx/passes/fake_tensor_prop.py",
      "description": "继承了fx.Interpreter，并在遍历fx graphs的过程中利用FakeTensor来记录Node的元信息，\nNOTE:也许可以用于中间计算图的构建",
      "line": 11,
      "title": "FakeTensorProp"
    },
    {
      "file": "torch/fx/passes/fake_tensor_prop.py",
      "description": "在每个node的meta字段处存储run_node返回的结果",
      "line": 32
    },
    {
      "file": "torch/fx/passes/fake_tensor_prop.py",
      "description": "除了使用了FakeTensorMode的上下文，其他与Interpreter的run方法区别不大",
      "line": 36
    },
    {
      "file": "torch/_subclasses/fake_tensor.py",
      "description": "继承了TorchDispatchMode，来使用FakeTensor的FakeTensorMode，关于FakeTensor，详见:https://pytorch.org/docs/stable/torch.compiler_fake_tensor.html",
      "line": 951,
      "title": "FakeTensorMode"
    },
    {
      "file": "torch/utils/_python_dispatch.py",
      "description": "FakeTensorMode的父类TorchDispatchMode，Dispatch理解为依据张量的metadata进行实际算子调用的逻辑，TorchDispatchMode的作用详见下面文档",
      "line": 14,
      "title": "TorchDispatchMode"
    },
    {
      "file": "torch/_subclasses/fake_tensor.py",
      "description": "FakeTensorMode覆盖父类的__torch_dispatch__方法的实现",
      "line": 985
    },
    {
      "file": "torch/_subclasses/fake_tensor.py",
      "description": "__torch_dispatch__方法的实际实现",
      "line": 992
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "用于CudaGraph中支持进行划分的Node的类",
      "line": 50
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "用于表示支持进行划分的fx Node的类，详见:https://pytorch.org/docs/stable/torch.compiler_transformations.html#capability-based-partitioner",
      "line": 10,
      "title": "CudaGraphsSupport"
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "提取经过FakeTensorProp处理后，保存在fx Node meta[\"val\"]中的FakeTensor",
      "line": 24
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "只要输入Node以及本Node的运行结果中存在一个非cuda张量，那么就不支持进行划分",
      "line": 32
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "利用Partitioner进行划分，详见:https://pytorch.org/docs/stable/torch.compiler_transformations.html#capability-based-partitioner",
      "line": 53
    },
    {
      "file": "torch/fx/passes/backends/cudagraphs.py",
      "description": "进行划分，并通过插入新的call_module node将划分的子图重新组成为graph module",
      "line": 54
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "将划分与重组后的graph module的各个sub module利用cuda graph包装",
      "line": 123,
      "title": "apply_cuda_graphs"
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "遍历Node，并寻找到之前划分的sub module Node，奖后将其利用CudaGraphModule包装，并替换掉原来的Node",
      "line": 125
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "包装graph module，并在cuda graph下运行它的实现类",
      "line": 26,
      "title": "CudaGraphModule"
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "直接覆盖__call__方法而不是nn.Module的forward方法来实现利用cuda graph运行的逻辑",
      "line": 44
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "复制静态的输入，replay cuda graph，然后将静态输入复制回去，并返回静态输出的克隆",
      "line": 49
    },
    {
      "file": "torch/_dynamo/backends/cudagraphs.py",
      "description": "replay与warm up的实现，比较的简单",
      "line": 58
    }
  ],
  "ref": "torchv2.0.1_dev",
  "description": "cudagraph后端优化的流程"
}